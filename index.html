<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Publication Archive">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Publication Archive</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./main.css">
</head>
<body>
	<table><tr><td rowspan="2" style="vertical-align: middle; padding-top: 6px;"></td><td style="padding-left: 10px;">
	<H1>Publication Archive</h1></td></tr><tr><td style="padding-left: 10px;">
		<button onclick="location.href='#2024';">2024</button>
		<button onclick="location.href='#2023';">2023</button>
		<button onclick="location.href='#2022';">2022</button>
		<button onclick="location.href='#2021';">2021</button>
		<button onclick="location.href='#2020';">2020</button>
	</td></tr></table>
<div class="content">
<div class="year_section">
<a name='2024'></a>
<h2 class="year_title">2024</h2>
    <ul class="publications">

<!--  Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://korrawe.github.io/dno-project/';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2024_DNO.png" alt="FishEyeVIT" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
DNO: Optimizing Diffusion Noise Can Serve As Universal Motion Priors
            </h2><div class="publication_authors">
Korrawe Karunratanakul, Konpat Preechakul, Emre Aksan, Thabo Beeler, Supasorn Suwajanakorn, Siyu Tang
            </div><div class="publication_venue">
CVPR 2024
            </div><div class="publication_abstract">
We propose Diffusion Noise Optimization (DNO), a new method that effectively leverages existing motion diffusion models as motion priors for a wide range of motion-related tasks.

Instead of training a task-specific diffusion model for each new task, DNO operates by optimizing the diffusion latent noise of an existing pre-trained text-to-motion model. Given the corresponding latent noise of a human motion, it propagates the gradient from the target criteria defined on the motion space through the whole denoising process to update the diffusion latent noise. As a result, DNO supports any use cases where criteria can be defined as a function of motion.

In particular, we show that, for motion editing and control, DNO outperforms existing methods in both achieving the objective and preserving the motion content. DNO accommodates a diverse range of editing modes, including changing trajectory, pose, joint locations, or avoiding newly added obstacles.

In addition, DNO is effective in motion denoising and completion, producing smooth and realistic motion from noisy and partial inputs. DNO achieves these results at inference time without the need for model retraining, offering great versatility for any defined reward or loss function on the motion representation.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->

<!--  Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://people.mpi-inf.mpg.de/~jianwang/projects/egowholemocap/index.html';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2024_FisheyeViT.png" alt="FishEyeVIT" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
Egocentric Whole-Body Motion Capture with FisheyeViT and Diffusion-Based Motion Refinement
            </h2><div class="publication_authors">
Jian Wang, Zhe Cao, Diogo Luvizon,   Lingjie Liu,  Kripasindhu Sarkar,   Danhang Tang,   Thabo Beeler,  Christian Theobalt
            </div><div class="publication_venue">
CVPR 2024
            </div><div class="publication_abstract">
In this work, we explore egocentric whole-body motion capture using a single fisheye camera, which simultaneously estimates human body and hand motion. This task presents significant challenges due to three factors, the lack of high-quality datasets, fisheye camera distortion, and human body self-occlusion. To address these challenges, we propose a novel approach that leverages FisheyeViT to extract fisheye image features, which are subsequently converted into pixel-aligned 3D heatmap representations for 3D human body pose prediction. For hand tracking, we incorporate dedicated hand detection and hand pose estimation networks for regressing 3D hand poses. Finally, we develop a diffusion-based whole-body motion prior model to refine the estimated whole-body motion while accounting for joint uncertainties. To train these networks, we collect a large synthetic dataset, EgoWholeBody, comprising 840,000 high-quality egocentric images captured across a diverse range of whole-body motion sequences. Quantitative and qualitative evaluations demonstrate the effectiveness of our method in producing high-quality whole-body motion estimates from a single egocentric camera.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->

<!--  Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://syntec-research.github.io/FaceFolds/';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2024_FaceFolds.png" alt="FaceFolds" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
FaceFolds: Meshed Radiance Manifolds for Efficient Volumetric Rendering of Dynamic Faces
            </h2><div class="publication_authors">
Safa C. Medin, Gengyan Li, Ruofei Du, Stephan Garbin, Philip Davidson, Gregory W. Wornell, Thabo Beeler, Abhimitra Meka
            </div><div class="publication_venue">
I3D 2024
            </div><div class="publication_abstract">
3D rendering of dynamic face captures is a challenging problem, and it demands improvements on several fronts‚Äîphotorealism, efficiency, compatibility, and configurability. We present a novel representation that enables high-quality volumetric rendering of an actor's dynamic facial performances with minimal compute and memory footprint. It runs natively on commodity graphics soft- and hardware, and allows for a graceful trade-off between quality and efficiency. Our method utilizes recent advances in neural rendering, particularly learning discrete radiance manifolds to sparsely sample the scene to model volumetric effects. We achieve efficient modeling by learning a single set of manifolds for the entire dynamic sequence, while implicitly modeling appearance changes as temporal canonical texture. We export a single layered mesh and view-independent RGBA texture video that is compatible with legacy graphics renderers without additional ML integration. We demonstrate our method by rendering dynamic face captures of real actors in a game engine, at comparable photorealism to state-of-the-art neural rendering techniques at previously unseen frame rates.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->
	    
<!--  Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://syntec-research.github.io/FaceStab/';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2024_FaceStab.png" alt="GANtlitz" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
Learning to Stabilize Faces
            </h2><div class="publication_authors">
Jan Bednarik, Erroll Wood, Vassilis Choutas, Timo Bolkart, Daoye Wang, Chenglei Wu, Thabo Beeler
            </div><div class="publication_venue">
EUROGRAPHICS 2024
            </div><div class="publication_abstract">
Modern facial scanning produces high-quality meshes but often requires subsequent stabilization to remove rigid head movement. This is crucial for game or movie character development where deformation due to expressions need be isolated from global head motion. Manual stabilization is tedious, leading to automated attempts. However, existing methods are flawed, requiring manual input, being imprecise, slow, or needing temporally consistent data. We propose a new learning-based approach which is precise and fully automatic. We frame stabilization as regression, use a 3DMM for training data generation and predict rigid transforms stabilizing pairs of misaligned face meshes. Experiments confirm our method's effectiveness for both random expression sets and facial performances.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->
	    
<!--  Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://syntec-research.github.io/GANtlitz/';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2024_GANtlitz.png" alt="GANtlitz" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
GANtlitz: Ultra High Resolution Generative Model for Multi-Modal Face Textures
            </h2><div class="publication_authors">
Aurel Gruber, Edo Collins, Abhimitra Meka, Franziska Mueller, Kripasindhu Sarkar, Sergio Orts-Escolano, Luca Prasso, Jay Busch, Markus Gross, Thabo Beeler
            </div><div class="publication_venue">
EUROGRAPHICS 2024
            </div><div class="publication_abstract">
High-resolution texture maps are essential to render photoreal digital humans for visual effects or to generate data for machine learning. The acquisi- tion of high resolution assets at scale is cumbersome, it involves enrolling a large number of human subjects, using expensive multi-view camera setups, and significant manual artistic effort to align the textures. To alleviate these problems, we introduce GANtlitz, a generative model that can synthesize multi-modal ultra-high-resolution face appearance maps for novel identities. Our method solves three distinct challenges: 1) unavailability of a very large data corpus generally required for training generative models, 2) memory and computational limitations of training a GAN at ultra-high resolutions, and 3) consistency of appearance features such as skin color, pores and wrinkles in high-resolution textures across different modalities. We introduce dual-style blocks, an extension to the style blocks of the StyleGAN2 architecture, which improve multi-modal synthesis. Our patch-based architecture is trained only on image patches obtained from a small set of face textures (&lt;100) and yet allows us to generate seamless appearance maps of novel identities at 6ùëò √ó 4ùëò resolution. Extensive qualitative and quantitative evaluations and baseline comparisons show the efficacy of our proposed system.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->
	    
<!--  Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://syntec-research.github.io/ShellNeRF/';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2024_ShellNeRF.png" alt="ShellNeRF" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
ShellNeRF: Learning a Controllable High-resolution Model of the Eye and Periocular Region
            </h2><div class="publication_authors">
Gengyan Li, Kripasindhu Sarkar, Abhimitra Meka, Marcel C. B√ºhler, Franziska Mueller, Paulo Gotardo, Otmar Hilliges, Thabo Beeler
            </div><div class="publication_venue">
EUROGRAPHICS 2024
            </div><div class="publication_abstract">
Eye gaze and expressions are crucial non-verbal signals in face-to-face communication. Visual effects and telepresence demand significant improvements in personalized tracking, animation, and synthesis of the eye region to achieve true immersion. Morphable face models, in combination with coordinate-based neural volumetric representations, show promise in solving the difficult problem of reconstructing intricate geometry (eyelashes) and synthesizing photorealistic appearance variations (wrinkles and specularities) of eye performances. We propose a novel hybrid representation - ShellNeRF - that builds a discretized volume around a 3DMM face mesh using concentric surfaces to model the deformable ‚Äòperiocular‚Äô region. We define a canonical space using the UV layout of the shells that constrains the space of dense correspondence search. Combined with an explicit eyeball mesh for modeling corneal light-transport, our model allows for animatable photorealistic 3D synthesis of the whole eye region. Using multi-view video input, we demonstrate significant improvements over state-of-the-art in expression re-enactment and transfer for high-resolution close-up views of the eye region.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->	

<!--  Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://vcai.mpi-inf.mpg.de/projects/MACS/';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2024_macs.png" alt="MACS" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
MACS: Mass Conditioned 3D Hand and Object Motion Synthesis
            </h2><div class="publication_authors">
Soshi Shimada, Franziska Mueller, Jan Bednarik, Bardia Doosti, Bernd Bickel, Danhang Tang, Vladislav Golyanik, Jonathan Taylor, Christian Theobalt, Thabo Beeler
            </div><div class="publication_venue">
3DV 2024
            </div><div class="publication_abstract">
The physical properties of an object, such as mass, significantly affect how we manipulate it with our hands. Surprisingly, this aspect has so far been neglected in prior work on 3D motion synthesis. To improve the naturalness of the synthesized 3D hand-object motions, this work proposes MACS---the first MAss Conditioned 3D hand and object motion Synthesis approach. Our approach is based on cascaded diffusion models and generates interactions that plausibly adjust based on the object's mass and interaction type. MACS also accepts a manually drawn 3D object trajectory as input and synthesizes the natural 3D hand motions conditioned by the object's mass. This flexibility enables MACS to be used for various downstream applications, such as generating synthetic training data for ML tasks, fast animation of hands for graphics workflows, and generating character interactions for computer games. We show experimentally that a small-scale dataset is sufficient for MACS to reasonably generalize across interpolated and extrapolated object masses unseen during the training. Furthermore, MACS shows moderate generalization to unseen objects, thanks to the mass-conditioned contact labels generated by our surface contact synthesis model ConNet. Our comprehensive user study confirms that the synthesized 3D hand-object interactions are highly plausible and realistic.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->	 
	  
<!--  Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://www.computationalimaging.org/publications/lsv/';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2024_lsv.png" alt="Layered Surface Volumes" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
Efficient 3D GANs with Layered Surface Volumes
            </h2><div class="publication_authors">
Yinghao Xu, Wang Yifan, Alexander W. Bergman, Menglei Chai, Bolei Zhou, Gordon Wetzstein
            </div><div class="publication_venue">
3DV 2024
            </div><div class="publication_abstract">
Access to high-quality and diverse 3D articulated digital human assets is crucial in various applications, ranging from virtual reality to social platforms. Generative approaches, such as 3D generative adversarial networks (GANs), are rapidly replacing laborious manual content creation tools. However, existing 3D GAN frameworks typically rely on scene representations that leverage either template meshes, which are fast but offer limited quality, or volumes, which offer high capacity but are slow to render, thereby limiting the 3D fidelity in GAN settings. In this work, we introduce layered surface volumes (LSVs) as a new 3D object representation for articulated digital humans. LSVs represent a human body using multiple textured mesh layers around a conventional template. These layers are rendered using alpha compositing with fast differentiable rasterization, and they can be interpreted as a volumetric representation that allocates its capacity to a manifold of finite thickness around the template. Unlike conventional single-layer templates that struggle with representing fine off-surface details like hair or accessories, our surface volumes naturally capture such details. LSVs can be articulated, and they exhibit exceptional efficiency in GAN settings, where a 2D generator learns to synthesize the RGBA textures for the individual layers. Trained on unstructured, single-view 2D image datasets, our LSV-GAN generates high-quality and view-consistent 3D articulated digital humans without the need for view-inconsistent 2D upsampling networks.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->	 
    </ul>
</div>
		
<div class="year_section">
<a name='2023'></a>
<h2 class="year_title">2023</h2>
    <ul class="publications">
		
<!--  Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://syntec-research.github.io/LitNeRF/';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2023_litnerf.png" alt="LitNeRF" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
LitNeRF: Intrinsic Radiance Decomposition for High-Quality View Synthesis and Relighting of Faces
            </h2><div class="publication_authors">
Kripasindhu Sarkar, Marcel C. B√ºhler, Gengyan Li, Daoye Wang, Delio Vicini, J√©r√©my Riviere, Yinda Zhang, Sergio Orts-Escolano, Paulo Gotardo, Thabo Beeler, Abhimitra Meka
            </div><div class="publication_venue">
SIGGRAPH Asia 2023
            </div><div class="publication_abstract">
High-fidelity, photorealistic 3D capture of a human face is a long-standing problem in computer graphics -- the complex material of skin, intricate geometry of hair, and fine scale textural details make it challenging. Traditional techniques rely on very large and expensive capture rigs to reconstruct explicit mesh geometry and appearance maps and require complex differentiable path-tracing to achieve photorealistic results. More recent volumetric methods (\eg, NeRFs) have enabled view-synthesis and sometimes relighting by learning an implicit representation of the density and reflectance basis, but suffer from artifacts and blurriness due to the inherent ambiguities in volumetric modeling. These problems are further exacerbated when capturing with few cameras and light sources. We present a novel technique for high-quality capture of a human face for 3D view synthesis and relighting using a sparse, compact capture rig consisting of 15 cameras and 15 lights. Our method combines a volumetric representation of the face reflectance with traditional multi-view stereo based geometry reconstruction. The proxy geometry allows us to anchor the 3D density field to prevent artifacts and guide the disentanglement of intrinsic radiance components of the face appearance such as diffuse and specular reflectance, and Direct Light Transport (shadowing) fields. Our hybrid representation significantly improves the state-of-the-art quality for arbitrarily dense renders of a face from desired camera viewpoint as well as environmental, directional, and near-field lighting.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->	
		
<!-- BEGIN Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://syntec-research.github.io/GroomGen/';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2023_groomgen.png" alt="GroomGen" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
GroomGen: A High-Quality Generative Hair Model Using Hierarchical Latent Representations
            </h2><div class="publication_authors">
Yuxiao Zhou, Menglei Chai, Alessandro Pepe, Markus Gross, Thabo Beeler
            </div><div class="publication_venue">
SIGGRAPH Asia 2023
            </div><div class="publication_abstract">
Despite recent successes in hair acquisition that fits a high-dimensional hair model to a specific input subject, generative hair models, which establish general embedding spaces for encoding, editing, and sampling diverse hairstyles, are way less explored. In this paper, we present GroomGen, the first generative model designed for hair geometry composed of highly-detailed dense strands. Our approach is motivated by two key ideas. First, we construct hair latent spaces covering both individual strands and hairstyles. The latent spaces are compact, expressive, and well-constrained for high-quality and diverse sampling. Second, we adopt a hierarchical hair representation that parameterizes a complete hair model to three levels: single strands, sparse guide hairs, and complete dense hairs. This representation is critical to the compactness of latent spaces, the robustness of training, and the efficiency of inference. Based on this hierarchical latent representation, our proposed pipeline consists of a strand-VAE and a hairstyle-VAE that encode an individual strand and a set of guide hairs to their respective latent spaces, a hybrid densification step that populates sparse guide hairs to a dense hair model, and a neural simulator that deforms hair driven by head pose. GroomGen not only enables novel hairstyle sampling and plausible hairstyle interpolation, but could also enable interactive editing of complex hairstyles, or serve as strong data-driven prior for hairstyle reconstruction from images. We demonstrate the superiority of our approach with qualitative examples of diverse sampled hairstyles and quantitative evaluation of generation quality regarding every single component and the entire pipeline.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->

<!-- BEGIN Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://syntec-research.github.io/Preface/';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2023_preface.png" alt="Preface" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
Preface: A Data-driven Volumetric Prior for Few-shot Ultra High-resolution Face Synthesis
            </h2><div class="publication_authors">
Marcel C. B√ºhler, Kripasindhu Sarkar, Tanmay Shah, Gengyan Li, Daoye Wang, Leonhard Helminger, Sergio Orts-Escolano, Dmitry Lagun, Otmar Hilliges, Thabo Beeler, Abhimitra Meka
            </div><div class="publication_venue">
ICCV 2023
            </div><div class="publication_abstract">
NeRFs have enabled highly realistic synthesis of human faces including complex appearance and reflectance effects of hair and skin. These methods typically require a large number of multi-view input images, making the process hardware intensive and cumbersome, limiting applicability to unconstrained settings. We propose a novel volumetric human face prior that enables the synthesis of ultra high-resolution novel views of subjects that are not part of the prior's training distribution. This prior model consists of an identity-conditioned NeRF, trained on a dataset of low-resolution multi-view images of diverse humans with known camera calibration. A simple sparse landmark-based 3D alignment of the training dataset allows our model to learn a smooth latent space of geometry and appearance despite a limited number of training identities. A high-quality volumetric representation of a novel subject can be obtained by model fitting to 2 or 3 camera views of arbitrary resolution. Importantly, our method requires as few as two views of casually captured images as input at inference time.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->	

<!-- BEGIN Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://arxiv.org/abs/2303.12865';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2023_nerfgan.png" alt="Nerf GAN" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
NeRF-GAN Distillation for Efficient 3D-Aware Generation with Convolutions
            </h2><div class="publication_authors">
Mohamad Shahbazi, Evangelos Ntavelis, Alessio Tonioni, Edo Collins, Danda Pani Paudel, Martin Danelljan, Luc Van Gool
            </div><div class="publication_venue">
ICCV 2023
            </div><div class="publication_abstract">
Pose-conditioned convolutional generative models struggle with high-quality 3D-consistent image generation from single-view datasets, due to their lack of sufficient 3D priors. Recently, the integration of Neural Radiance Fields (NeRFs) and generative models, such as Generative Adversarial Networks (GANs), has transformed 3D-aware generation from single-view images. NeRF-GANs exploit the strong inductive bias of 3D neural representations and volumetric rendering at the cost of higher computational complexity. This study aims at revisiting pose-conditioned 2D GANs for efficient 3D-aware generation at inference time by distilling 3D knowledge from pretrained NeRF-GANS. We propose a simple and effective method, based on re-using the well-disentangled latent space of a pre-trained NeRF-GAN in a pose-conditioned convolutional network to directly generate 3D-consistent images corresponding to the underlying 3D representations. Experiments on several datasets demonstrate that the proposed method obtains results comparable with volumetric rendering in terms of quality and 3D consistency while benefiting from the superior computational advantage of convolutional networks.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->	

<!-- BEGIN Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://eldentse.github.io/Spectral-Graphormer/';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2023_graphormer.png" alt="Spectral Graphormer" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
Spectral Graphormer: Spectral Graph-based Transformer for Egocentric Two-Hand Reconstruction using Multi-View Color Images
            </h2><div class="publication_authors">
Tze Ho Elden Tse, Franziska Mueller, Zhengyang Shen, Danhang Tang, Thabo Beeler, Mingsong Dou, Yinda Zhang, Sasa Petrovic, Hyung Jin Chang, Jonathan Taylor, Bardia Doosti
            </div><div class="publication_venue">
ICCV 2023
            </div><div class="publication_abstract">
We propose a novel transformer-based framework that reconstructs two high fidelity hands from multi-view RGB images. Unlike existing hand pose estimation methods, where one typically trains a deep network to regress hand model parameters from single RGB image, we consider a more challenging problem setting where we directly regress the absolute root poses of two-hands with extended forearm at high resolution from egocentric view. As existing datasets are either infeasible for egocentric viewpoints or lack background variations, we create a large-scale synthetic dataset with diverse scenarios and collect a real dataset from multi-calibrated camera setup to verify our proposed multi-view image feature fusion strategy. To make the reconstruction physically plausible, we propose two strategies: (i) a coarse-to-fine spectral graph convolution decoder to smoothen the meshes during upsampling and (ii) an optimisation-based refinement stage at inference to prevent self-penetrations. Through extensive quantitative and qualitative evaluations, we show that our framework is able to produce realistic two-hand reconstructions and demonstrate the generalisation of synthetic-trained models to real data, as well as real-time AR/VR applications.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->	

<!-- BEGIN Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://zkf1997.github.io/DIMOS/';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2023_dimos.png" alt="DIMOS" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
DIMOS: Synthesizing Diverse Human Motions in 3D Indoor Scenes
            </h2><div class="publication_authors">
Kaifeng Zhao,  Yan Zhang,  Shaofei Wang,  Thabo Beeler,  Siyu Tang
            </div><div class="publication_venue">
ICCV 2023
            </div><div class="publication_abstract">
We present a novel method for populating 3D indoor scenes with virtual humans that can navigate in the environment and interact with objects in a realistic manner. Existing approaches rely on training sequences that contain captured human motions and the 3D scenes they interact with. However, such interaction data are costly, difficult to capture, and can hardly cover all plausible human-scene interactions in complex environments. To address these challenges, we propose a reinforcement learning-based approach that enables virtual humans to navigate in 3D scenes and interact with objects realistically and autonomously, driven by learned motion control policies. The motion control policies employ latent motion action spaces, which correspond to realistic motion primitives and are learned from large-scale motion capture data using a powerful generative motion model. For navigation in a 3D environment, we propose a scene-aware policy with novel state and reward designs for collision avoidance. Combined with navigation mesh-based path-finding algorithms to generate intermediate waypoints, our approach enables the synthesis of diverse human motions navigating in 3D indoor scenes and avoiding obstacles. To generate fine-grained human-object interactions, we carefully curate interaction goal guidance using a marker-based body representation and leverage features based on the signed distance field (SDF) to encode human-scene proximity relations. Our method can synthesize realistic and diverse human-object interactions (e.g.,~sitting on a chair and then getting up) even for out-of-distribution test scenarios with different object shapes, orientations, starting body positions, and poses. Experimental results demonstrate that our approach outperforms state-of-the-art methods in terms of both motion naturalness and diversity.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->	

<!-- BEGIN Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://czhang0528.github.io/iti-gen';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2023_itigen.png" alt="ITIGen" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
ITI-GEN: Inclusive Text-to-Image Generation
            </h2><div class="publication_authors">
Cheng Zhang,    Xuanbai Chen,    Siqi Chai,    Chen Henry Wu,    Dmitry Lagun,
Thabo Beeler,    Fernando De la Torre
            </div><div class="publication_venue">
ICCV 2023 (Oral, Best Paper Finalist)
            </div><div class="publication_abstract">
Text-to-image generative models often reflect the biases of the training data, leading to unequal representations of underrepresented groups. This study investigates inclusive text-to-image generative models that generate images based on human-written prompts and ensure the resulting images are uniformly distributed across attributes of interest. Unfortunately, directly expressing the desired attributes in the prompt often leads to sub-optimal results due to linguistic ambiguity or model misrepresentation.

Hence, this paper proposes a drastically different approach that adheres to the maxim that "a picture is worth a thousand words". We show that, for some attributes, images can represent concepts more expressively than text. For instance, categories of skin tones are typically hard to specify by text but can be easily represented by example images. Building upon these insights, we propose a novel approach, ITI-GEN, that leverages readily available reference images for Inclusive Text-to-Image GENeration. The key idea is learning a set of prompt embeddings to generate images that can effectively represent all desired attribute categories. More importantly, ITI-GEN requires no model fine-tuning, making it computationally efficient to augment existing text-to-image models. Extensive experiments demonstrate that ITI-GEN largely improves over state-of-the-art models to generate inclusive images from a prompt.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->	

<!-- BEGIN Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://vcai.mpi-inf.mpg.de/projects/DragGAN/';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2023_draggan.png" alt="DragGAN" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold
            </h2><div class="publication_authors">
Xingang Pan,   Ayush Tewari,   Thomas Leimk√ºhler,  Lingjie Liu,   Abhimitra Meka,   Christian Theobalt
            </div><div class="publication_venue">
SIGGRAPH 2023
            </div><div class="publication_abstract">
Synthesizing visual content that meets users' needs often requires flexible and precise controllability of the pose, shape, expression, and layout of the generated objects. Existing approaches gain controllability of generative adversarial networks (GANs) via manually annotated training data or a prior 3D model, which often lack flexibility, precision, and generality. In this work, we study a powerful yet much less explored way of controlling GANs, that is, to "drag" any points of the image to precisely reach target points in a user-interactive manner, as shown in Fig.1. To achieve this, we propose DragGAN, which consists of two main components including: 1) a feature-based motion supervision that drives the handle point to move towards the target position, and 2) a new point tracking approach that leverages the discriminative GAN features to keep localizing the position of the handle points. Through DragGAN, anyone can deform an image with precise control over where pixels go, thus manipulating the pose, shape, expression, and layout of diverse categories such as animals, cars, humans, landscapes, etc. As these manipulations are performed on the learned generative image manifold of a GAN, they tend to produce realistic outputs even for challenging scenarios such as hallucinating occluded content and deforming shapes that consistently follow the object's rigidity. Both qualitative and quantitative comparisons demonstrate the advantage of DragGAN over prior approaches in the tasks of image manipulation and point tracking. We also showcase the manipulation of real images through GAN inversion.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->	

<!-- BEGIN Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://people.mpi-inf.mpg.de/~jianwang/projects/sceneego/index.htm';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2023_poseEstimation.png" alt="PoseEstimation" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
Scene-aware Egocentric 3D Human Pose Estimation
            </h2><div class="publication_authors">
Jian Wang,   Diogo Luvizon,   Weipeng Xu,   Lingjie Liu,   Kripasindhu Sarkar,   Christian Theobalt
            </div><div class="publication_venue">
CVPR 2023
            </div><div class="publication_abstract">
Egocentric 3D human pose estimation with a single head-mounted fisheye camera has recently attracted attention due to its numerous applications in virtual and augmented reality. Existing methods still struggle in challenging poses where the human body is highly occluded or is closely interacting with the scene. To address this issue, we propose a scene-aware egocentric pose estimation method that guides the prediction of the egocentric pose with scene constraints. To this end, we propose an egocentric depth estimation network to predict the scene depth map from a wide-view egocentric fisheye camera while mitigating the occlusion of the human body with a depth-inpainting network. Next, we propose a scene-aware pose estimation network that projects the 2D image features and estimated depth map of the scene into a voxel space and regresses the 3D pose with a V2V network. The voxel-based feature representation provides the direct geometric connection between 2D image features and scene geometry, and further facilitates the V2V network to constrain the predicted pose based on the estimated scene geometry. To enable the training of the aforementioned networks, we also generated a synthetic dataset, called EgoGTA, and an in-the-wild dataset based on EgoPW, called EgoPW-Scene. The experimental results of our new evaluation sequences show that the predicted 3D egocentric poses are accurate and physically plausible in terms of human-scene interaction, demonstrating that our method outperforms the state-of-the-art methods both quantitatively and qualitatively.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->	

<!-- BEGIN Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://augmentedperception.github.io/monoavatar/';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2023_monoavatar.png" alt="MonoAvatar" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
MonoAvatar: Learning Personalized High Quality Volumetric Head Avatars from Monocular RGB Videos
            </h2><div class="publication_authors">
Ziqian Bai, Feitong Tan, Zeng Huang, Kripasindhu Sarkar, Danhang Tang, Di Qiu, Abhimitra Meka, Ruofei Du, Mingsong Dou, Sergio Orts-Escolano, Rohit Pandey, Ping Tan, Thabo Beeler, Sean Fanello, Yinda Zhang
            </div><div class="publication_venue">
CVPR 2023
            </div><div class="publication_abstract">
We propose a method to learn a high-quality implicit 3D head avatar from a monocular RGB video captured in the wild. The learnt avatar is driven by a parametric face model to achieve user-controlled facial expressions and head poses. Our hybrid pipeline combines the geometry prior and dynamic tracking of a 3DMM with a neural radiance field to achieve fine-grained control and photorealism. To reduce over-smoothing and improve out-of-model expressions synthesis, we propose to predict local features anchored on the 3DMM geometry. These learnt features are driven by 3DMM deformation and interpolated in 3D space to yield the volumetric radiance at a designated query point. We further show that using a Convolutional Neural Network in the UV space is critical in incorporating spatial context and producing representative local features. Extensive experiments show that we are able to reconstruct high-quality avatars, with more accurate expression-dependent details, good generalization to out-of-training expressions, and quantitatively superior renderings compared to other state-of-the-art approaches.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->	
	  
	</ul>
</div>

<div class="year_section">
<a name='2022'></a>
<h2 class="year_title">2022</h2>
    <ul class="publications">
<!--  Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://ligethz.github.io/eyenerf_website/';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2022_eyenerf.png" alt="Eye NeRF" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
EyeNeRF: A Hybrid Representation for Photorealistic Synthesis, Animation and Relighting of Human Eyes
            </h2><div class="publication_authors">
Gengyan Li, Abhimitra Meka, Franziska M√ºller, Marcel C. B√ºhler, Otmar Hilliges, Thabo Beeler
            </div><div class="publication_venue">
SIGGRAPH 2022
            </div><div class="publication_abstract">
A unique challenge in creating high-quality animatable and relightable 3D avatars of real people is modeling human eyes, particularly in conjunction with the surrounding periocular face region. The challenge of synthesizing eyes is multifold as it requires 1) appropriate representations for the various components of the eye and the periocular region for coherent viewpoint synthesis, capable of representing diffuse, refractive and highly reflective surfaces, 2) disentangling skin and eye appearance from environmental illumination such that it may be rendered under novel lighting conditions, and 3) capturing eyeball motion and the deformation of the surrounding skin to enable re-gazing.

These challenges have traditionally necessitated the use of expensive and cumbersome capture setups to obtain high-quality results, and even then, modeling of the full eye region holistically has remained elusive. We present a novel geometry and appearance representation that enables high-fidelity capture and photorealistic animation, view synthesis and relighting of the eye region using only a sparse set of lights and cameras. Our hybrid representation combines an explicit parametric surface model for the eyeball surface with implicit deformable volumetric representations for the periocular region and the interior of the eye. This novel hybrid model has been designed specifically to address the various parts of that exceptionally challenging facial area - the explicit eyeball surface allows modeling refraction and high frequency specular reflection at the cornea, whereas the implicit representation is well suited to model lower frequency skin reflection via spherical harmonics and can represent non-surface structures such as hair (i.e. eyebrows) or highly diffuse volumetric bodies (i.e. sclera), both of which are a challenge for explicit surface models. Tightly integrating the two representations in a joint framework allows controlled photoreal image synthesis and joint optimization of both the geometry parameters of the eyeball and the implicit neural network in continuous 3D space. We show that for high-resolution close-ups of the human eye, our model can synthesize high-fidelity animated gaze from novel views under unseen illumination conditions, allowing to generate visually rich eye imagery.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->	 
	  
<!--  Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://augmentedperception.github.io/voluxgan/';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2022_voluxgan.png" alt="VoluxGAN" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
VoLux-GAN: A Generative Model for 3D Face Synthesis with HDRI Relighting
            </h2><div class="publication_authors">
Feitong Tan, Sean Fanello, Abhimitra Meka, Sergio Orts-Escolano, Danhang Tang, Rohit Pandey, Jonathan Taylor, Ping Tan, Yinda Zhang
            </div><div class="publication_venue">
SIGGRAPH 2022
            </div><div class="publication_abstract">
We propose VoLux-GAN, a generative framework to synthesize 3D-aware faces with convincing relighting. Our main contribution is a volumetric HDRI relighting method that can efficiently accumulate albedo, diffuse and specular lighting contributions along each 3D ray for any desired HDR environmental map. Additionally, we show the importance of supervising the image decomposition process using multiple discriminators. In particular, we propose a data augmentation technique that leverages recent advances in single image portrait relighting to enforce consistent geometry, albedo, diffuse and specular components. Multiple experiments and comparisons with other generative frameworks show how our model is a step forward towards photorealistic relightable generative models.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->	 
	  
<!--  Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://zkf1997.github.io/COINS/index.html';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2022_coins.png" alt="COINS" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
COINS: Compositional Human-Scene Interaction Synthesis with Semantic Control
            </h2><div class="publication_authors">
Kaifeng Zhao,  Shaofei Wang,  Yan Zhang,  Thabo Beeler,  Siyu Tang
            </div><div class="publication_venue">
ECCV 2022
            </div><div class="publication_abstract">
The physical properties of an object, such as mass, significantly affect how we manipulate it with our hands. Surprisingly, this aspect has so far been neglected in prior work on 3D motion synthesis. To improve the naturalness of the synthesized 3D hand-object motions, this work proposes MACS---the first MAss Conditioned 3D hand and object motion Synthesis approach. Our approach is based on cascaded diffusion models and generates interactions that plausibly adjust based on the object's mass and interaction type. MACS also accepts a manually drawn 3D object trajectory as input and synthesizes the natural 3D hand motions conditioned by the object's mass. This flexibility enables MACS to be used for various downstream applications, such as generating synthetic training data for ML tasks, fast animation of hands for graphics workflows, and generating character interactions for computer games. We show experimentally that a small-scale dataset is sufficient for MACS to reasonably generalize across interpolated and extrapolated object masses unseen during the training. Furthermore, MACS shows moderate generalization to unseen objects, thanks to the mass-conditioned contact labels generated by our surface contact synthesis model ConNet. Our comprehensive user study confirms that the synthesized 3D hand-object interactions are highly plausible and realistic.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->	 
	  
<!--  Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://handtracker.mpi-inf.mpg.de/projects/HandFlow/';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2022_handFlow.png" alt="HandFlow" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
HandFlow: Quantifying View-Dependent 3D Ambiguity in Two-Hand Reconstruction with Normalizing Flow
            </h2><div class="publication_authors">
Jiayi Wang, Diogo Luvizon, Franziska Mueller, Florian Bernard, Adam Kortylewski, Dan Casas, Christian Theobalt
            </div><div class="publication_venue">
VMV 2022
            </div><div class="publication_abstract">
Reconstructing two-hand interactions from a single image is a challenging problem due to ambiguities that stem from projective geometry and heavy occlusions. Existing methods are designed to estimate only a single pose, despite the fact that there exist other valid reconstructions that fit the image evidence equally well. In this paper we propose to address this issue by explicitly modeling the distribution of plausible reconstructions in a conditional normalizing flow framework. This allows us to directly supervise the posterior distribution through a novel determinant magnitude regularization, which is key to varied 3D hand pose samples that project well into the input image. We also demonstrate that metrics commonly used to assess reconstruction quality are insufficient to evaluate pose predictions under such severe ambiguity. To address this, we release the first dataset with multiple plausible annotations per image called MultiHands. The additional annotations enable us to evaluate the estimated distribution using the maximum mean discrepancy metric. Through this, we demonstrate the quality of our probabilistic reconstruction and show that explicit ambiguity modeling is better-suited for this challenging problem.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->	 
	</ul>
</div>



<div class="year_section">
<a name='2021'></a>
<h2 class="year_title">2021</h2>
    <ul class="publications">
<!--  Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://mcbuehler.github.io/VariTex/';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2021_varitex.png" alt="VariTex" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
VariTex: Variational Neural Face Textures
            </h2><div class="publication_authors">
Marcel C. B√ºhler,    Abhimitra Meka,    Gengyan Li,    Thabo Beeler,    Otmar Hilliges
            </div><div class="publication_venue">
ICCV 2021
            </div><div class="publication_abstract">
Deep generative models can synthesize photorealistic images of human faces with novel identities. However, a key challenge to the wide applicability of such techniques is to provide independent control over semantically meaningful parameters: appearance, head pose, face shape, and facial expressions. In this paper, we propose VariTex - to the best of our knowledge the first method that learns a variational latent feature space of neural face textures, which allows sampling of novel identities. We combine this generative model with a parametric face model and gain explicit control over head pose and facial expressions. To generate complete images of human heads, we propose an additive decoder that adds plausible details such as hair. A novel training scheme enforces a pose-independent latent space and in consequence, allows learning a one-to-many mapping between latent codes and pose-conditioned exterior regions. The resulting method can generate geometrically consistent images of novel identities under fine-grained control over head pose, face shape, and facial expressions. This facilitates a broad range of downstream tasks, like sampling novel identities, changing the head pose, expression transfer, and more.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->	 
	  
<!--  Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://4dqv.mpi-inf.mpg.de/EventHands/';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2021_eventHands.png" alt="EventHands" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
EventHands: Real-Time Neural 3D Hand Pose Estimation from an Event Stream
            </h2><div class="publication_authors">
Viktor Rudnev, Vladislav Golyanik, Jiayi Wang, Hans-Peter Seidel, Franziska Mueller, Mohamed Elgharib, Christian Theobalt
            </div><div class="publication_venue">
ICCV 2021
            </div><div class="publication_abstract">
3D hand pose estimation from monocular videos is a long-standing and challenging problem, which is now seeing a strong upturn. In this work, we address it for the first time using a single event camera, i.e., an asynchronous vision sensor reacting on brightness changes. Our EventHands approach has characteristics previously not demonstrated with a single RGB or depth camera such as high temporal resolution at low data throughputs and real-time performance at 1000 Hz. Due to the different data modality of event cameras compared to classical cameras, existing methods cannot be directly applied to and re-trained for event streams. We thus design a new neural approach which accepts a new event stream representation suitable for learning, which is trained on newly-generated synthetic event streams and can generalise to real data. Experiments show that EventHands outperforms recent monocular methods using a colour (or depth) camera in terms of accuracy and its ability to capture hand motions of unprecedented speed. Our method, the event stream simulator and the dataset are publicly available.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->	 
	  
<!--  Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://people.mpi-inf.mpg.de/~mhaberma/projects/2021-deepphyscloth/';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2021_physcloth.png" alt="Physics Aware Cloth" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
Deep Physics-aware Inference of Cloth Deformation for Monocular Human Performance Capture
            </h2><div class="publication_authors">
Yue Li,   Marc Habermann,   Bernhard Thomaszewski,   Stelian Coros,   Thabo Beeler,   Christian Theobalt
            </div><div class="publication_venue">
3DV 2021
            </div><div class="publication_abstract">
Recent monocular human performance capture approaches have shown compelling dense tracking results of the full body from a single RGB camera. However, existing methods either do not estimate clothing at all or model cloth deformation with simple geometric priors instead of taking into account the underlying physical principles. This leads to noticeable artifacts in their reconstructions, \eg baked-in wrinkles, implausible deformations that seemingly defy gravity, and intersections between cloth and body. To address these problems, we propose a person-specific, learning-based method that integrates a simulation layer into the training process to provide for the first time physics supervision in the context of weakly supervised deep monocular human performance capture. We show how integrating physics into the training process improves the learned cloth deformations, allows modeling clothing as a separate piece of geometry, and largely reduces cloth-body intersections. Relying only on weak 2D multi-view supervision during training, our approach leads to a significant improvement over current state-of-the-art methods and is thus a clear step towards realistic monocular capture of the entire deforming surface of a clothed human.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->	 
	</ul>
</div>



<div class="year_section">
<a name='2020'></a>
<h2 class="year_title">2020</h2>
    <ul class="publications">
<!--  Entry -->	
      <li class="publication_entry">
        <div class="publication" onclick="location.href='https://vcai.mpi-inf.mpg.de/projects/DeepRelightableTextures/';" style="cursor: pointer;">
          <div class="publication_image">
            <img src="./images/2020_relight.png" alt="Relightable Textures" />
          </div>
          <div class="publication_content">
            <h2 class="publication_title">
Deep Relightable Textures
            </h2><div class="publication_authors">
Abhimitra Meka,   Rohit Pandey,   Christian Haene,   Sergio Orts-Escolano,  
Peter Barnum,   Philip Davidson,   Daniel Erickson,   Yinda Zhang,   Jonathan Taylor,   Sofien Bouaziz,  
Chloe Legendre,   Wan-Chun Ma,   Ryan Overbeck,   Thabo Beeler,   Paul Debevec,  
Shahram Izadi,   Christian Theobalt, Christoph Rhemann,   Sean Fanello
            </div><div class="publication_venue">
SIGGRAPH Asia 2020
            </div><div class="publication_abstract">
The increasing demand for 3D content in augmented and virtual reality has motivated the development of volumetric performance capture systems such as the Light Stage. Recent advances are pushing free viewpoint relightable videos of dynamic human performances closer to photorealistic quality. However, despite significant efforts, these sophisticated systems are limited by reconstruction and rendering algorithms which do not fully model complex 3D structures and higher order light transport effects such as global illumination and sub-surface scattering. In this paper, we propose a system that combines traditional geometric pipelines with a neural rendering scheme to generate photorealistic renderings of dynamic performances under desired viewpoint and lighting. Our system leverages deep neural networks that model the classical rendering process to learn implicit features that represent the view-dependent appearance of the subject independent of the geometry layout, allowing for generalization to unseen subject poses and even novel subject identity. Detailed experiments and comparisons demonstrate the efficacy and versatility of our method to generate high-quality results, significantly outperforming the existing state-of-the-art solutions.
            </div>
          </div>
        </div>
      </li>
<!-- END Entry -->	 
	</ul>
</div>
</div>
</body>
</html>
